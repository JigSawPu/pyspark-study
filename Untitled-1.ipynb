{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/06/16 19:21:30 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession, Row\n",
    "import json\n",
    "from pyspark.sql.functions import rand\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/06/16 11:47:35 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define the schema of the data to write to Kafka\n",
    "schema = StructType([\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"age\", StringType(), True)\n",
    "])\n",
    "\n",
    "spark = SparkSession \\\n",
    "          .builder \\\n",
    "          .appName(\"APP\") \\\n",
    "          .getOrCreate()\n",
    "\n",
    "# Read the data from a source (e.g. a file)\n",
    "source_data = spark.read.csv(\"file.csv\", header=True, schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[name: string, age: string]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_data = source_data.withColumn(\"timestamp\", current_timestamp())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+--------------------+\n",
      "|name|                 age|           timestamp|\n",
      "+----+--------------------+--------------------+\n",
      "|   0|  0.4967141530112327|2023-06-16 11:48:...|\n",
      "|   1|  0.6476885381006925|2023-06-16 11:48:...|\n",
      "|   2|-0.23415337472333597|2023-06-16 11:48:...|\n",
      "|   3|  1.5792128155073915|2023-06-16 11:48:...|\n",
      "|   4| -0.4694743859349521|2023-06-16 11:48:...|\n",
      "|   5|-0.46341769281246226|2023-06-16 11:48:...|\n",
      "|   6| 0.24196227156603412|2023-06-16 11:48:...|\n",
      "|   7| -1.7249178325130328|2023-06-16 11:48:...|\n",
      "|   8| -1.0128311203344238|2023-06-16 11:48:...|\n",
      "|   9| -0.9080240755212109|2023-06-16 11:48:...|\n",
      "|  10|   1.465648768921554|2023-06-16 11:48:...|\n",
      "|  11| 0.06752820468792384|2023-06-16 11:48:...|\n",
      "|  12| -0.5443827245251827|2023-06-16 11:48:...|\n",
      "|  13| -1.1509935774223028|2023-06-16 11:48:...|\n",
      "|  14|  -0.600638689918805|2023-06-16 11:48:...|\n",
      "|  15| -0.6017066122293969|2023-06-16 11:48:...|\n",
      "|  16|-0.01349722473793...|2023-06-16 11:48:...|\n",
      "|  17|   0.822544912103189|2023-06-16 11:48:...|\n",
      "|  18|  0.2088635950047554|2023-06-16 11:48:...|\n",
      "|  19| -1.3281860488984305|2023-06-16 11:48:...|\n",
      "+----+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/06/16 11:48:50 WARN CSVHeaderChecker: Number of column in CSV header is not equal to number of fields in the schema:\n",
      " Header length: 3, schema size: 2\n",
      "CSV file: file:///home/aryakumar/Downloads/file.csv\n"
     ]
    }
   ],
   "source": [
    "transformed_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "|{\"name\":\"0\",\"age\"...|\n",
      "|{\"name\":\"1\",\"age\"...|\n",
      "|{\"name\":\"2\",\"age\"...|\n",
      "|{\"name\":\"3\",\"age\"...|\n",
      "|{\"name\":\"4\",\"age\"...|\n",
      "|{\"name\":\"5\",\"age\"...|\n",
      "|{\"name\":\"6\",\"age\"...|\n",
      "|{\"name\":\"7\",\"age\"...|\n",
      "|{\"name\":\"8\",\"age\"...|\n",
      "|{\"name\":\"9\",\"age\"...|\n",
      "|{\"name\":\"10\",\"age...|\n",
      "|{\"name\":\"11\",\"age...|\n",
      "|{\"name\":\"12\",\"age...|\n",
      "|{\"name\":\"13\",\"age...|\n",
      "|{\"name\":\"14\",\"age...|\n",
      "|{\"name\":\"15\",\"age...|\n",
      "|{\"name\":\"16\",\"age...|\n",
      "|{\"name\":\"17\",\"age...|\n",
      "|{\"name\":\"18\",\"age...|\n",
      "|{\"name\":\"19\",\"age...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/06/16 11:49:12 WARN CSVHeaderChecker: Number of column in CSV header is not equal to number of fields in the schema:\n",
      " Header length: 3, schema size: 2\n",
      "CSV file: file:///home/aryakumar/Downloads/file.csv\n"
     ]
    }
   ],
   "source": [
    "transformed_data.selectExpr(\"to_json(struct(*)) AS value\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "js = open('file.json')\n",
    "data = json.load(js)\n",
    "df = spark.createDataFrame(data[:1000])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "|{\"x\":0.49671414,\"...|\n",
      "|{\"x\":0.64768857,\"...|\n",
      "|{\"x\":-0.23415338,...|\n",
      "|{\"x\":1.5792128,\"y...|\n",
      "|{\"x\":-0.46947438,...|\n",
      "|{\"x\":-0.46341768,...|\n",
      "|{\"x\":0.24196227,\"...|\n",
      "|{\"x\":-1.7249179,\"...|\n",
      "|{\"x\":-1.0128311,\"...|\n",
      "|{\"x\":-0.9080241,\"...|\n",
      "|{\"x\":1.4656488,\"y...|\n",
      "|{\"x\":0.0675282,\"y...|\n",
      "|{\"x\":-0.54438275,...|\n",
      "|{\"x\":-1.1509936,\"...|\n",
      "|{\"x\":-0.6006387,\"...|\n",
      "|{\"x\":-0.6017066,\"...|\n",
      "|{\"x\":-0.013497225...|\n",
      "|{\"x\":0.82254493,\"...|\n",
      "|{\"x\":0.2088636,\"y...|\n",
      "|{\"x\":-1.328186,\"y...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(to_json(struct(\"x\", \"y\")).alias(\"value\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- x: float (nullable = true)\n",
      " |-- y: float (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.selectExpr(\"CAST(x AS FLOAT)\", \"CAST(y AS FLOAT)\"); df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+\n",
      "|           x|          y|\n",
      "+------------+-----------+\n",
      "|  0.49671414| -0.1382643|\n",
      "|  0.64768857|  1.5230298|\n",
      "| -0.23415338|-0.23413695|\n",
      "|   1.5792128|  0.7674347|\n",
      "| -0.46947438| 0.54256004|\n",
      "| -0.46341768|-0.46572974|\n",
      "|  0.24196227| -1.9132802|\n",
      "|  -1.7249179| -0.5622875|\n",
      "|  -1.0128311| 0.31424734|\n",
      "|  -0.9080241| -1.4123037|\n",
      "|   1.4656488| -0.2257763|\n",
      "|   0.0675282| -1.4247482|\n",
      "| -0.54438275| 0.11092259|\n",
      "|  -1.1509936| 0.37569803|\n",
      "|  -0.6006387|-0.29169375|\n",
      "|  -0.6017066|  1.8522782|\n",
      "|-0.013497225| -1.0577109|\n",
      "|  0.82254493| -1.2208437|\n",
      "|   0.2088636| -1.9596701|\n",
      "|   -1.328186| 0.19686124|\n",
      "+------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dfCoresettree import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_df = df.rdd.toDF().toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- x: double (nullable = true)\n",
      " |-- y: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pandas_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfg = df.rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.496714</td>\n",
       "      <td>-0.138264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.647689</td>\n",
       "      <td>1.523030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.234153</td>\n",
       "      <td>-0.234137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.579213</td>\n",
       "      <td>0.767435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.469474</td>\n",
       "      <td>0.542560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>0.800409</td>\n",
       "      <td>0.754291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>1.188913</td>\n",
       "      <td>0.708304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>0.351448</td>\n",
       "      <td>1.070150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>-0.026521</td>\n",
       "      <td>-0.881875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>-0.163067</td>\n",
       "      <td>-0.744903</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            x         y\n",
       "0    0.496714 -0.138264\n",
       "1    0.647689  1.523030\n",
       "2   -0.234153 -0.234137\n",
       "3    1.579213  0.767435\n",
       "4   -0.469474  0.542560\n",
       "..        ...       ...\n",
       "995  0.800409  0.754291\n",
       "996  1.188913  0.708304\n",
       "997  0.351448  1.070150\n",
       "998 -0.026521 -0.881875\n",
       "999 -0.163067 -0.744903\n",
       "\n",
       "[1000 rows x 2 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.createDataFrame(dfg).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_schema = StructType([\n",
    "    StructField(\"x\", StringType()),\n",
    "    StructField(\"y\", StringType())\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = [\"{'x': '-2.1356742139786955', 'y': '3.1377485336599937'}\", \"{'x': '0.655900776529761', 'y': '0.19473619471877485'}\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'x': '-2.1356742139786955', 'y': '3.137748533...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'x': '0.655900776529761', 'y': '0.19473619471...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0\n",
       "0  {'x': '-2.1356742139786955', 'y': '3.137748533...\n",
       "1  {'x': '0.655900776529761', 'y': '0.19473619471..."
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.DataFrame(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "aa = spark.createDataFrame(pd.DataFrame(arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------------+\n",
      "|                  x|                 y|\n",
      "+-------------------+------------------+\n",
      "|-2.1356742139786955|3.1377485336599937|\n",
      "+-------------------+------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "aa.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- x: string (nullable = true)\n",
      " |-- y: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "aa.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aa.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = \"struct<x:double,y:double>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "aa = aa.select(from_json(col(\"0\"), json_schema).alias(\"parsed\")).selectExpr(\"parsed.*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-2.1356742139786955</td>\n",
       "      <td>3.1377485336599937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.655900776529761</td>\n",
       "      <td>0.19473619471877485</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     x                    y\n",
       "0  -2.1356742139786955   3.1377485336599937\n",
       "1    0.655900776529761  0.19473619471877485"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aa.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- 0: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "aa.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aa.select(from_json(aa.value, json_schema).alias(\"parsed\")).selectExpr(\"parsed.*\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Define the data\n",
    "data = [(\"{'x': '-2.1356742139786955', 'y': '3.1377485336599937'}\",),\n",
    "        (\"{'x': '1.0', 'y': '2.0'}\",),\n",
    "        (\"{'x': '-3.0', 'y': '4.0'}\",)]\n",
    "\n",
    "# Create the DataFrame\n",
    "df = spark.createDataFrame(data, [\"coordinates\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|         coordinates|\n",
      "+--------------------+\n",
      "|{'x': '-2.1356742...|\n",
      "|{'x': '1.0', 'y':...|\n",
      "|{'x': '-3.0', 'y'...|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- coordinates: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = \"struct<x:double,y:double>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+\n",
      "|   x|   y|\n",
      "+----+----+\n",
      "|null|null|\n",
      "|null|null|\n",
      "|null|null|\n",
      "+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.select(from_json(col(\"coordinates\"), schema).alias(\"coordinates\"))\n",
    "\n",
    "# Extract the x and y values\n",
    "df = df.select(col(\"coordinates.x\").alias(\"x\"), col(\"coordinates.y\").alias(\"y\"))\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_schema = StructType([\n",
    "    StructField(\"x\", StringType())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+\n",
      "|                  x|                  y|\n",
      "+-------------------+-------------------+\n",
      "|-2.1356742139786955| 3.1377485336599937|\n",
      "|  0.655900776529761|0.19473619471877485|\n",
      "+-------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.json(spark.sparkContext.parallelize(arr))\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.selectExpr(\"CAST(x AS FLOAT)\", \"CAST(y AS FLOAT)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- x: float (nullable = true)\n",
      " |-- y: float (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/home/aryakumar/Downloads/spark-3.4.0-bin-hadoop3/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/aryakumar/.ivy2/cache\n",
      "The jars for the packages stored in: /home/aryakumar/.ivy2/jars\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-7c3979cb-61b1-41e2-8a89-7d6c2824c5c9;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.4.0 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.4.0 in central\n",
      "\tfound org.apache.kafka#kafka-clients;3.3.2 in central\n",
      "\tfound org.lz4#lz4-java;1.8.0 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.9.1 in central\n",
      "\tfound org.slf4j#slf4j-api;2.0.6 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-runtime;3.3.4 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-api;3.3.4 in central\n",
      "\tfound commons-logging#commons-logging;1.1.3 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.0 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.11.1 in central\n",
      ":: resolution report :: resolve 597ms :: artifacts dl 18ms\n",
      "\t:: modules in use:\n",
      "\tcom.google.code.findbugs#jsr305;3.0.0 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.1.3 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.11.1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-api;3.3.4 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-runtime;3.3.4 from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;3.3.2 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.4.0 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.4.0 from central in [default]\n",
      "\torg.lz4#lz4-java;1.8.0 from central in [default]\n",
      "\torg.slf4j#slf4j-api;2.0.6 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.9.1 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   11  |   0   |   0   |   0   ||   11  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-7c3979cb-61b1-41e2-8a89-7d6c2824c5c9\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 11 already retrieved (0kB/12ms)\n",
      "23/06/16 10:13:13 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/06/16 10:13:14 INFO SparkContext: Running Spark version 3.4.0\n",
      "23/06/16 10:13:14 INFO ResourceUtils: ==============================================================\n",
      "23/06/16 10:13:14 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
      "23/06/16 10:13:14 INFO ResourceUtils: ==============================================================\n",
      "23/06/16 10:13:14 INFO SparkContext: Submitted application: APP\n",
      "23/06/16 10:13:14 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
      "23/06/16 10:13:14 INFO ResourceProfile: Limiting resource is cpu\n",
      "23/06/16 10:13:14 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
      "23/06/16 10:13:14 INFO SecurityManager: Changing view acls to: aryakumar\n",
      "23/06/16 10:13:14 INFO SecurityManager: Changing modify acls to: aryakumar\n",
      "23/06/16 10:13:14 INFO SecurityManager: Changing view acls groups to: \n",
      "23/06/16 10:13:14 INFO SecurityManager: Changing modify acls groups to: \n",
      "23/06/16 10:13:14 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: aryakumar; groups with view permissions: EMPTY; users with modify permissions: aryakumar; groups with modify permissions: EMPTY\n",
      "23/06/16 10:13:14 INFO Utils: Successfully started service 'sparkDriver' on port 44333.\n",
      "23/06/16 10:13:15 INFO SparkEnv: Registering MapOutputTracker\n",
      "23/06/16 10:13:15 INFO SparkEnv: Registering BlockManagerMaster\n",
      "23/06/16 10:13:15 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "23/06/16 10:13:15 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
      "23/06/16 10:13:15 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "23/06/16 10:13:15 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-78ee9c2f-91e3-46c1-a01d-413cb5cc7c75\n",
      "23/06/16 10:13:15 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB\n",
      "23/06/16 10:13:15 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "23/06/16 10:13:15 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI\n",
      "23/06/16 10:13:15 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
      "23/06/16 10:13:15 INFO SparkContext: Added JAR file:///home/aryakumar/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.4.0.jar at spark://fedora:44333/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.4.0.jar with timestamp 1686890594591\n",
      "23/06/16 10:13:15 INFO SparkContext: Added JAR file:///home/aryakumar/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.4.0.jar at spark://fedora:44333/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.4.0.jar with timestamp 1686890594591\n",
      "23/06/16 10:13:15 INFO SparkContext: Added JAR file:///home/aryakumar/.ivy2/jars/org.apache.kafka_kafka-clients-3.3.2.jar at spark://fedora:44333/jars/org.apache.kafka_kafka-clients-3.3.2.jar with timestamp 1686890594591\n",
      "23/06/16 10:13:15 INFO SparkContext: Added JAR file:///home/aryakumar/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar at spark://fedora:44333/jars/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1686890594591\n",
      "23/06/16 10:13:15 INFO SparkContext: Added JAR file:///home/aryakumar/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar at spark://fedora:44333/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1686890594591\n",
      "23/06/16 10:13:15 INFO SparkContext: Added JAR file:///home/aryakumar/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar at spark://fedora:44333/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar with timestamp 1686890594591\n",
      "23/06/16 10:13:15 INFO SparkContext: Added JAR file:///home/aryakumar/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar at spark://fedora:44333/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1686890594591\n",
      "23/06/16 10:13:15 INFO SparkContext: Added JAR file:///home/aryakumar/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.9.1.jar at spark://fedora:44333/jars/org.xerial.snappy_snappy-java-1.1.9.1.jar with timestamp 1686890594591\n",
      "23/06/16 10:13:15 INFO SparkContext: Added JAR file:///home/aryakumar/.ivy2/jars/org.slf4j_slf4j-api-2.0.6.jar at spark://fedora:44333/jars/org.slf4j_slf4j-api-2.0.6.jar with timestamp 1686890594591\n",
      "23/06/16 10:13:15 INFO SparkContext: Added JAR file:///home/aryakumar/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar at spark://fedora:44333/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar with timestamp 1686890594591\n",
      "23/06/16 10:13:15 INFO SparkContext: Added JAR file:///home/aryakumar/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar at spark://fedora:44333/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1686890594591\n",
      "23/06/16 10:13:15 INFO SparkContext: Added file file:///home/aryakumar/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.4.0.jar at file:///home/aryakumar/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.4.0.jar with timestamp 1686890594591\n",
      "23/06/16 10:13:15 INFO Utils: Copying /home/aryakumar/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.4.0.jar to /tmp/spark-416fadf0-bf87-4a91-ba94-d124616a0ed7/userFiles-d5ece133-635f-4991-92f4-99896f5099b8/org.apache.spark_spark-sql-kafka-0-10_2.12-3.4.0.jar\n",
      "23/06/16 10:13:15 INFO SparkContext: Added file file:///home/aryakumar/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.4.0.jar at file:///home/aryakumar/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.4.0.jar with timestamp 1686890594591\n",
      "23/06/16 10:13:15 INFO Utils: Copying /home/aryakumar/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.4.0.jar to /tmp/spark-416fadf0-bf87-4a91-ba94-d124616a0ed7/userFiles-d5ece133-635f-4991-92f4-99896f5099b8/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.4.0.jar\n",
      "23/06/16 10:13:15 INFO SparkContext: Added file file:///home/aryakumar/.ivy2/jars/org.apache.kafka_kafka-clients-3.3.2.jar at file:///home/aryakumar/.ivy2/jars/org.apache.kafka_kafka-clients-3.3.2.jar with timestamp 1686890594591\n",
      "23/06/16 10:13:15 INFO Utils: Copying /home/aryakumar/.ivy2/jars/org.apache.kafka_kafka-clients-3.3.2.jar to /tmp/spark-416fadf0-bf87-4a91-ba94-d124616a0ed7/userFiles-d5ece133-635f-4991-92f4-99896f5099b8/org.apache.kafka_kafka-clients-3.3.2.jar\n",
      "23/06/16 10:13:15 INFO SparkContext: Added file file:///home/aryakumar/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar at file:///home/aryakumar/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1686890594591\n",
      "23/06/16 10:13:15 INFO Utils: Copying /home/aryakumar/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar to /tmp/spark-416fadf0-bf87-4a91-ba94-d124616a0ed7/userFiles-d5ece133-635f-4991-92f4-99896f5099b8/com.google.code.findbugs_jsr305-3.0.0.jar\n",
      "23/06/16 10:13:15 INFO SparkContext: Added file file:///home/aryakumar/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar at file:///home/aryakumar/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1686890594591\n",
      "23/06/16 10:13:15 INFO Utils: Copying /home/aryakumar/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar to /tmp/spark-416fadf0-bf87-4a91-ba94-d124616a0ed7/userFiles-d5ece133-635f-4991-92f4-99896f5099b8/org.apache.commons_commons-pool2-2.11.1.jar\n",
      "23/06/16 10:13:15 INFO SparkContext: Added file file:///home/aryakumar/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar at file:///home/aryakumar/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar with timestamp 1686890594591\n",
      "23/06/16 10:13:15 INFO Utils: Copying /home/aryakumar/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar to /tmp/spark-416fadf0-bf87-4a91-ba94-d124616a0ed7/userFiles-d5ece133-635f-4991-92f4-99896f5099b8/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar\n",
      "23/06/16 10:13:15 INFO SparkContext: Added file file:///home/aryakumar/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar at file:///home/aryakumar/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1686890594591\n",
      "23/06/16 10:13:15 INFO Utils: Copying /home/aryakumar/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar to /tmp/spark-416fadf0-bf87-4a91-ba94-d124616a0ed7/userFiles-d5ece133-635f-4991-92f4-99896f5099b8/org.lz4_lz4-java-1.8.0.jar\n",
      "23/06/16 10:13:15 INFO SparkContext: Added file file:///home/aryakumar/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.9.1.jar at file:///home/aryakumar/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.9.1.jar with timestamp 1686890594591\n",
      "23/06/16 10:13:15 INFO Utils: Copying /home/aryakumar/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.9.1.jar to /tmp/spark-416fadf0-bf87-4a91-ba94-d124616a0ed7/userFiles-d5ece133-635f-4991-92f4-99896f5099b8/org.xerial.snappy_snappy-java-1.1.9.1.jar\n",
      "23/06/16 10:13:15 INFO SparkContext: Added file file:///home/aryakumar/.ivy2/jars/org.slf4j_slf4j-api-2.0.6.jar at file:///home/aryakumar/.ivy2/jars/org.slf4j_slf4j-api-2.0.6.jar with timestamp 1686890594591\n",
      "23/06/16 10:13:15 INFO Utils: Copying /home/aryakumar/.ivy2/jars/org.slf4j_slf4j-api-2.0.6.jar to /tmp/spark-416fadf0-bf87-4a91-ba94-d124616a0ed7/userFiles-d5ece133-635f-4991-92f4-99896f5099b8/org.slf4j_slf4j-api-2.0.6.jar\n",
      "23/06/16 10:13:15 INFO SparkContext: Added file file:///home/aryakumar/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar at file:///home/aryakumar/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar with timestamp 1686890594591\n",
      "23/06/16 10:13:15 INFO Utils: Copying /home/aryakumar/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar to /tmp/spark-416fadf0-bf87-4a91-ba94-d124616a0ed7/userFiles-d5ece133-635f-4991-92f4-99896f5099b8/org.apache.hadoop_hadoop-client-api-3.3.4.jar\n",
      "23/06/16 10:13:15 INFO SparkContext: Added file file:///home/aryakumar/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar at file:///home/aryakumar/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1686890594591\n",
      "23/06/16 10:13:15 INFO Utils: Copying /home/aryakumar/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar to /tmp/spark-416fadf0-bf87-4a91-ba94-d124616a0ed7/userFiles-d5ece133-635f-4991-92f4-99896f5099b8/commons-logging_commons-logging-1.1.3.jar\n",
      "23/06/16 10:13:15 INFO Executor: Starting executor ID driver on host fedora\n",
      "23/06/16 10:13:15 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''\n",
      "23/06/16 10:13:15 INFO Executor: Fetching file:///home/aryakumar/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.9.1.jar with timestamp 1686890594591\n",
      "23/06/16 10:13:15 INFO Utils: /home/aryakumar/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.9.1.jar has been previously copied to /tmp/spark-416fadf0-bf87-4a91-ba94-d124616a0ed7/userFiles-d5ece133-635f-4991-92f4-99896f5099b8/org.xerial.snappy_snappy-java-1.1.9.1.jar\n",
      "23/06/16 10:13:15 INFO Executor: Fetching file:///home/aryakumar/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar with timestamp 1686890594591\n",
      "23/06/16 10:13:15 INFO Utils: /home/aryakumar/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar has been previously copied to /tmp/spark-416fadf0-bf87-4a91-ba94-d124616a0ed7/userFiles-d5ece133-635f-4991-92f4-99896f5099b8/org.apache.hadoop_hadoop-client-api-3.3.4.jar\n",
      "23/06/16 10:13:15 INFO Executor: Fetching file:///home/aryakumar/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.4.0.jar with timestamp 1686890594591\n",
      "23/06/16 10:13:15 INFO Utils: /home/aryakumar/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.4.0.jar has been previously copied to /tmp/spark-416fadf0-bf87-4a91-ba94-d124616a0ed7/userFiles-d5ece133-635f-4991-92f4-99896f5099b8/org.apache.spark_spark-sql-kafka-0-10_2.12-3.4.0.jar\n",
      "23/06/16 10:13:15 INFO Executor: Fetching file:///home/aryakumar/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1686890594591\n",
      "23/06/16 10:13:15 INFO Utils: /home/aryakumar/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar has been previously copied to /tmp/spark-416fadf0-bf87-4a91-ba94-d124616a0ed7/userFiles-d5ece133-635f-4991-92f4-99896f5099b8/org.lz4_lz4-java-1.8.0.jar\n",
      "23/06/16 10:13:15 INFO Executor: Fetching file:///home/aryakumar/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1686890594591\n",
      "23/06/16 10:13:15 INFO Utils: /home/aryakumar/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar has been previously copied to /tmp/spark-416fadf0-bf87-4a91-ba94-d124616a0ed7/userFiles-d5ece133-635f-4991-92f4-99896f5099b8/commons-logging_commons-logging-1.1.3.jar\n",
      "23/06/16 10:13:15 INFO Executor: Fetching file:///home/aryakumar/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.4.0.jar with timestamp 1686890594591\n",
      "23/06/16 10:13:15 INFO Utils: /home/aryakumar/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.4.0.jar has been previously copied to /tmp/spark-416fadf0-bf87-4a91-ba94-d124616a0ed7/userFiles-d5ece133-635f-4991-92f4-99896f5099b8/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.4.0.jar\n",
      "23/06/16 10:13:15 INFO Executor: Fetching file:///home/aryakumar/.ivy2/jars/org.apache.kafka_kafka-clients-3.3.2.jar with timestamp 1686890594591\n",
      "23/06/16 10:13:15 INFO Utils: /home/aryakumar/.ivy2/jars/org.apache.kafka_kafka-clients-3.3.2.jar has been previously copied to /tmp/spark-416fadf0-bf87-4a91-ba94-d124616a0ed7/userFiles-d5ece133-635f-4991-92f4-99896f5099b8/org.apache.kafka_kafka-clients-3.3.2.jar\n",
      "23/06/16 10:13:15 INFO Executor: Fetching file:///home/aryakumar/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1686890594591\n",
      "23/06/16 10:13:15 INFO Utils: /home/aryakumar/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar has been previously copied to /tmp/spark-416fadf0-bf87-4a91-ba94-d124616a0ed7/userFiles-d5ece133-635f-4991-92f4-99896f5099b8/org.apache.commons_commons-pool2-2.11.1.jar\n",
      "23/06/16 10:13:15 INFO Executor: Fetching file:///home/aryakumar/.ivy2/jars/org.slf4j_slf4j-api-2.0.6.jar with timestamp 1686890594591\n",
      "23/06/16 10:13:15 INFO Utils: /home/aryakumar/.ivy2/jars/org.slf4j_slf4j-api-2.0.6.jar has been previously copied to /tmp/spark-416fadf0-bf87-4a91-ba94-d124616a0ed7/userFiles-d5ece133-635f-4991-92f4-99896f5099b8/org.slf4j_slf4j-api-2.0.6.jar\n",
      "23/06/16 10:13:15 INFO Executor: Fetching file:///home/aryakumar/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar with timestamp 1686890594591\n",
      "23/06/16 10:13:15 INFO Utils: /home/aryakumar/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar has been previously copied to /tmp/spark-416fadf0-bf87-4a91-ba94-d124616a0ed7/userFiles-d5ece133-635f-4991-92f4-99896f5099b8/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar\n",
      "23/06/16 10:13:15 INFO Executor: Fetching file:///home/aryakumar/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1686890594591\n",
      "23/06/16 10:13:15 INFO Utils: /home/aryakumar/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar has been previously copied to /tmp/spark-416fadf0-bf87-4a91-ba94-d124616a0ed7/userFiles-d5ece133-635f-4991-92f4-99896f5099b8/com.google.code.findbugs_jsr305-3.0.0.jar\n",
      "23/06/16 10:13:15 INFO Executor: Fetching spark://fedora:44333/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar with timestamp 1686890594591\n",
      "23/06/16 10:13:15 INFO TransportClientFactory: Successfully created connection to fedora/172.20.10.2:44333 after 19 ms (0 ms spent in bootstraps)\n",
      "23/06/16 10:13:15 INFO Utils: Fetching spark://fedora:44333/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar to /tmp/spark-416fadf0-bf87-4a91-ba94-d124616a0ed7/userFiles-d5ece133-635f-4991-92f4-99896f5099b8/fetchFileTemp3576628362732026135.tmp\n",
      "23/06/16 10:13:15 INFO Utils: /tmp/spark-416fadf0-bf87-4a91-ba94-d124616a0ed7/userFiles-d5ece133-635f-4991-92f4-99896f5099b8/fetchFileTemp3576628362732026135.tmp has been previously copied to /tmp/spark-416fadf0-bf87-4a91-ba94-d124616a0ed7/userFiles-d5ece133-635f-4991-92f4-99896f5099b8/org.apache.hadoop_hadoop-client-api-3.3.4.jar\n",
      "23/06/16 10:13:15 INFO Executor: Adding file:/tmp/spark-416fadf0-bf87-4a91-ba94-d124616a0ed7/userFiles-d5ece133-635f-4991-92f4-99896f5099b8/org.apache.hadoop_hadoop-client-api-3.3.4.jar to class loader\n",
      "23/06/16 10:13:15 INFO Executor: Fetching spark://fedora:44333/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.4.0.jar with timestamp 1686890594591\n",
      "23/06/16 10:13:15 INFO Utils: Fetching spark://fedora:44333/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.4.0.jar to /tmp/spark-416fadf0-bf87-4a91-ba94-d124616a0ed7/userFiles-d5ece133-635f-4991-92f4-99896f5099b8/fetchFileTemp4530840029096355031.tmp\n",
      "23/06/16 10:13:15 INFO Utils: /tmp/spark-416fadf0-bf87-4a91-ba94-d124616a0ed7/userFiles-d5ece133-635f-4991-92f4-99896f5099b8/fetchFileTemp4530840029096355031.tmp has been previously copied to /tmp/spark-416fadf0-bf87-4a91-ba94-d124616a0ed7/userFiles-d5ece133-635f-4991-92f4-99896f5099b8/org.apache.spark_spark-sql-kafka-0-10_2.12-3.4.0.jar\n",
      "23/06/16 10:13:15 INFO Executor: Adding file:/tmp/spark-416fadf0-bf87-4a91-ba94-d124616a0ed7/userFiles-d5ece133-635f-4991-92f4-99896f5099b8/org.apache.spark_spark-sql-kafka-0-10_2.12-3.4.0.jar to class loader\n",
      "23/06/16 10:13:15 INFO Executor: Fetching spark://fedora:44333/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1686890594591\n",
      "23/06/16 10:13:15 INFO Utils: Fetching spark://fedora:44333/jars/org.lz4_lz4-java-1.8.0.jar to /tmp/spark-416fadf0-bf87-4a91-ba94-d124616a0ed7/userFiles-d5ece133-635f-4991-92f4-99896f5099b8/fetchFileTemp8315280797991663276.tmp\n",
      "23/06/16 10:13:15 INFO Utils: /tmp/spark-416fadf0-bf87-4a91-ba94-d124616a0ed7/userFiles-d5ece133-635f-4991-92f4-99896f5099b8/fetchFileTemp8315280797991663276.tmp has been previously copied to /tmp/spark-416fadf0-bf87-4a91-ba94-d124616a0ed7/userFiles-d5ece133-635f-4991-92f4-99896f5099b8/org.lz4_lz4-java-1.8.0.jar\n",
      "23/06/16 10:13:15 INFO Executor: Adding file:/tmp/spark-416fadf0-bf87-4a91-ba94-d124616a0ed7/userFiles-d5ece133-635f-4991-92f4-99896f5099b8/org.lz4_lz4-java-1.8.0.jar to class loader\n",
      "23/06/16 10:13:15 INFO Executor: Fetching spark://fedora:44333/jars/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1686890594591\n",
      "23/06/16 10:13:15 INFO Utils: Fetching spark://fedora:44333/jars/com.google.code.findbugs_jsr305-3.0.0.jar to /tmp/spark-416fadf0-bf87-4a91-ba94-d124616a0ed7/userFiles-d5ece133-635f-4991-92f4-99896f5099b8/fetchFileTemp8186735623125782884.tmp\n",
      "23/06/16 10:13:15 INFO Utils: /tmp/spark-416fadf0-bf87-4a91-ba94-d124616a0ed7/userFiles-d5ece133-635f-4991-92f4-99896f5099b8/fetchFileTemp8186735623125782884.tmp has been previously copied to /tmp/spark-416fadf0-bf87-4a91-ba94-d124616a0ed7/userFiles-d5ece133-635f-4991-92f4-99896f5099b8/com.google.code.findbugs_jsr305-3.0.0.jar\n",
      "23/06/16 10:13:15 INFO Executor: Adding file:/tmp/spark-416fadf0-bf87-4a91-ba94-d124616a0ed7/userFiles-d5ece133-635f-4991-92f4-99896f5099b8/com.google.code.findbugs_jsr305-3.0.0.jar to class loader\n",
      "23/06/16 10:13:15 INFO Executor: Fetching spark://fedora:44333/jars/org.xerial.snappy_snappy-java-1.1.9.1.jar with timestamp 1686890594591\n",
      "23/06/16 10:13:15 INFO Utils: Fetching spark://fedora:44333/jars/org.xerial.snappy_snappy-java-1.1.9.1.jar to /tmp/spark-416fadf0-bf87-4a91-ba94-d124616a0ed7/userFiles-d5ece133-635f-4991-92f4-99896f5099b8/fetchFileTemp8438287936580795173.tmp\n",
      "23/06/16 10:13:15 INFO Utils: /tmp/spark-416fadf0-bf87-4a91-ba94-d124616a0ed7/userFiles-d5ece133-635f-4991-92f4-99896f5099b8/fetchFileTemp8438287936580795173.tmp has been previously copied to /tmp/spark-416fadf0-bf87-4a91-ba94-d124616a0ed7/userFiles-d5ece133-635f-4991-92f4-99896f5099b8/org.xerial.snappy_snappy-java-1.1.9.1.jar\n",
      "23/06/16 10:13:15 INFO Executor: Adding file:/tmp/spark-416fadf0-bf87-4a91-ba94-d124616a0ed7/userFiles-d5ece133-635f-4991-92f4-99896f5099b8/org.xerial.snappy_snappy-java-1.1.9.1.jar to class loader\n",
      "23/06/16 10:13:15 INFO Executor: Fetching spark://fedora:44333/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar with timestamp 1686890594591\n",
      "23/06/16 10:13:15 INFO Utils: Fetching spark://fedora:44333/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar to /tmp/spark-416fadf0-bf87-4a91-ba94-d124616a0ed7/userFiles-d5ece133-635f-4991-92f4-99896f5099b8/fetchFileTemp18267381913522428535.tmp\n",
      "23/06/16 10:13:15 INFO Utils: /tmp/spark-416fadf0-bf87-4a91-ba94-d124616a0ed7/userFiles-d5ece133-635f-4991-92f4-99896f5099b8/fetchFileTemp18267381913522428535.tmp has been previously copied to /tmp/spark-416fadf0-bf87-4a91-ba94-d124616a0ed7/userFiles-d5ece133-635f-4991-92f4-99896f5099b8/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar\n",
      "23/06/16 10:13:15 INFO Executor: Adding file:/tmp/spark-416fadf0-bf87-4a91-ba94-d124616a0ed7/userFiles-d5ece133-635f-4991-92f4-99896f5099b8/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar to class loader\n",
      "23/06/16 10:13:15 INFO Executor: Fetching spark://fedora:44333/jars/org.slf4j_slf4j-api-2.0.6.jar with timestamp 1686890594591\n",
      "23/06/16 10:13:15 INFO Utils: Fetching spark://fedora:44333/jars/org.slf4j_slf4j-api-2.0.6.jar to /tmp/spark-416fadf0-bf87-4a91-ba94-d124616a0ed7/userFiles-d5ece133-635f-4991-92f4-99896f5099b8/fetchFileTemp14388296739471279819.tmp\n",
      "23/06/16 10:13:15 INFO Utils: /tmp/spark-416fadf0-bf87-4a91-ba94-d124616a0ed7/userFiles-d5ece133-635f-4991-92f4-99896f5099b8/fetchFileTemp14388296739471279819.tmp has been previously copied to /tmp/spark-416fadf0-bf87-4a91-ba94-d124616a0ed7/userFiles-d5ece133-635f-4991-92f4-99896f5099b8/org.slf4j_slf4j-api-2.0.6.jar\n",
      "23/06/16 10:13:15 INFO Executor: Adding file:/tmp/spark-416fadf0-bf87-4a91-ba94-d124616a0ed7/userFiles-d5ece133-635f-4991-92f4-99896f5099b8/org.slf4j_slf4j-api-2.0.6.jar to class loader\n",
      "23/06/16 10:13:15 INFO Executor: Fetching spark://fedora:44333/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.4.0.jar with timestamp 1686890594591\n",
      "23/06/16 10:13:15 INFO Utils: Fetching spark://fedora:44333/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.4.0.jar to /tmp/spark-416fadf0-bf87-4a91-ba94-d124616a0ed7/userFiles-d5ece133-635f-4991-92f4-99896f5099b8/fetchFileTemp12182962347329552721.tmp\n",
      "23/06/16 10:13:15 INFO Utils: /tmp/spark-416fadf0-bf87-4a91-ba94-d124616a0ed7/userFiles-d5ece133-635f-4991-92f4-99896f5099b8/fetchFileTemp12182962347329552721.tmp has been previously copied to /tmp/spark-416fadf0-bf87-4a91-ba94-d124616a0ed7/userFiles-d5ece133-635f-4991-92f4-99896f5099b8/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.4.0.jar\n",
      "23/06/16 10:13:15 INFO Executor: Adding file:/tmp/spark-416fadf0-bf87-4a91-ba94-d124616a0ed7/userFiles-d5ece133-635f-4991-92f4-99896f5099b8/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.4.0.jar to class loader\n",
      "23/06/16 10:13:15 INFO Executor: Fetching spark://fedora:44333/jars/org.apache.kafka_kafka-clients-3.3.2.jar with timestamp 1686890594591\n",
      "23/06/16 10:13:15 INFO Utils: Fetching spark://fedora:44333/jars/org.apache.kafka_kafka-clients-3.3.2.jar to /tmp/spark-416fadf0-bf87-4a91-ba94-d124616a0ed7/userFiles-d5ece133-635f-4991-92f4-99896f5099b8/fetchFileTemp10316529326685207811.tmp\n",
      "23/06/16 10:13:15 INFO Utils: /tmp/spark-416fadf0-bf87-4a91-ba94-d124616a0ed7/userFiles-d5ece133-635f-4991-92f4-99896f5099b8/fetchFileTemp10316529326685207811.tmp has been previously copied to /tmp/spark-416fadf0-bf87-4a91-ba94-d124616a0ed7/userFiles-d5ece133-635f-4991-92f4-99896f5099b8/org.apache.kafka_kafka-clients-3.3.2.jar\n",
      "23/06/16 10:13:15 INFO Executor: Adding file:/tmp/spark-416fadf0-bf87-4a91-ba94-d124616a0ed7/userFiles-d5ece133-635f-4991-92f4-99896f5099b8/org.apache.kafka_kafka-clients-3.3.2.jar to class loader\n",
      "23/06/16 10:13:15 INFO Executor: Fetching spark://fedora:44333/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1686890594591\n",
      "23/06/16 10:13:15 INFO Utils: Fetching spark://fedora:44333/jars/commons-logging_commons-logging-1.1.3.jar to /tmp/spark-416fadf0-bf87-4a91-ba94-d124616a0ed7/userFiles-d5ece133-635f-4991-92f4-99896f5099b8/fetchFileTemp14294738965395238912.tmp\n",
      "23/06/16 10:13:15 INFO Utils: /tmp/spark-416fadf0-bf87-4a91-ba94-d124616a0ed7/userFiles-d5ece133-635f-4991-92f4-99896f5099b8/fetchFileTemp14294738965395238912.tmp has been previously copied to /tmp/spark-416fadf0-bf87-4a91-ba94-d124616a0ed7/userFiles-d5ece133-635f-4991-92f4-99896f5099b8/commons-logging_commons-logging-1.1.3.jar\n",
      "23/06/16 10:13:15 INFO Executor: Adding file:/tmp/spark-416fadf0-bf87-4a91-ba94-d124616a0ed7/userFiles-d5ece133-635f-4991-92f4-99896f5099b8/commons-logging_commons-logging-1.1.3.jar to class loader\n",
      "23/06/16 10:13:15 INFO Executor: Fetching spark://fedora:44333/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1686890594591\n",
      "23/06/16 10:13:15 INFO Utils: Fetching spark://fedora:44333/jars/org.apache.commons_commons-pool2-2.11.1.jar to /tmp/spark-416fadf0-bf87-4a91-ba94-d124616a0ed7/userFiles-d5ece133-635f-4991-92f4-99896f5099b8/fetchFileTemp18413740696120507356.tmp\n",
      "23/06/16 10:13:15 INFO Utils: /tmp/spark-416fadf0-bf87-4a91-ba94-d124616a0ed7/userFiles-d5ece133-635f-4991-92f4-99896f5099b8/fetchFileTemp18413740696120507356.tmp has been previously copied to /tmp/spark-416fadf0-bf87-4a91-ba94-d124616a0ed7/userFiles-d5ece133-635f-4991-92f4-99896f5099b8/org.apache.commons_commons-pool2-2.11.1.jar\n",
      "23/06/16 10:13:15 INFO Executor: Adding file:/tmp/spark-416fadf0-bf87-4a91-ba94-d124616a0ed7/userFiles-d5ece133-635f-4991-92f4-99896f5099b8/org.apache.commons_commons-pool2-2.11.1.jar to class loader\n",
      "23/06/16 10:13:15 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 40889.\n",
      "23/06/16 10:13:15 INFO NettyBlockTransferService: Server created on fedora:40889\n",
      "23/06/16 10:13:15 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "23/06/16 10:13:15 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, fedora, 40889, None)\n",
      "23/06/16 10:13:15 INFO BlockManagerMasterEndpoint: Registering block manager fedora:40889 with 434.4 MiB RAM, BlockManagerId(driver, fedora, 40889, None)\n",
      "23/06/16 10:13:15 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, fedora, 40889, None)\n",
      "23/06/16 10:13:15 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, fedora, 40889, None)\n",
      "23/06/16 10:13:16 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\n",
      "23/06/16 10:13:16 INFO SharedState: Warehouse path is 'file:/home/aryakumar/Downloads/spark-warehouse'.\n",
      "23/06/16 10:13:17 INFO CodeGenerator: Code generated in 261.468154 ms\n",
      "23/06/16 10:13:17 INFO CodeGenerator: Code generated in 45.86508 ms\n",
      "23/06/16 10:13:19 INFO AdminClientConfig: AdminClientConfig values: \n",
      "\tbootstrap.servers = [localhost:9092]\n",
      "\tclient.dns.lookup = use_all_dns_ips\n",
      "\tclient.id = \n",
      "\tconnections.max.idle.ms = 300000\n",
      "\tdefault.api.timeout.ms = 60000\n",
      "\tmetadata.max.age.ms = 300000\n",
      "\tmetric.reporters = []\n",
      "\tmetrics.num.samples = 2\n",
      "\tmetrics.recording.level = INFO\n",
      "\tmetrics.sample.window.ms = 30000\n",
      "\treceive.buffer.bytes = 65536\n",
      "\treconnect.backoff.max.ms = 1000\n",
      "\treconnect.backoff.ms = 50\n",
      "\trequest.timeout.ms = 30000\n",
      "\tretries = 2147483647\n",
      "\tretry.backoff.ms = 100\n",
      "\tsasl.client.callback.handler.class = null\n",
      "\tsasl.jaas.config = null\n",
      "\tsasl.kerberos.kinit.cmd = /usr/bin/kinit\n",
      "\tsasl.kerberos.min.time.before.relogin = 60000\n",
      "\tsasl.kerberos.service.name = null\n",
      "\tsasl.kerberos.ticket.renew.jitter = 0.05\n",
      "\tsasl.kerberos.ticket.renew.window.factor = 0.8\n",
      "\tsasl.login.callback.handler.class = null\n",
      "\tsasl.login.class = null\n",
      "\tsasl.login.connect.timeout.ms = null\n",
      "\tsasl.login.read.timeout.ms = null\n",
      "\tsasl.login.refresh.buffer.seconds = 300\n",
      "\tsasl.login.refresh.min.period.seconds = 60\n",
      "\tsasl.login.refresh.window.factor = 0.8\n",
      "\tsasl.login.refresh.window.jitter = 0.05\n",
      "\tsasl.login.retry.backoff.max.ms = 10000\n",
      "\tsasl.login.retry.backoff.ms = 100\n",
      "\tsasl.mechanism = GSSAPI\n",
      "\tsasl.oauthbearer.clock.skew.seconds = 30\n",
      "\tsasl.oauthbearer.expected.audience = null\n",
      "\tsasl.oauthbearer.expected.issuer = null\n",
      "\tsasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000\n",
      "\tsasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000\n",
      "\tsasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100\n",
      "\tsasl.oauthbearer.jwks.endpoint.url = null\n",
      "\tsasl.oauthbearer.scope.claim.name = scope\n",
      "\tsasl.oauthbearer.sub.claim.name = sub\n",
      "\tsasl.oauthbearer.token.endpoint.url = null\n",
      "\tsecurity.protocol = PLAINTEXT\n",
      "\tsecurity.providers = null\n",
      "\tsend.buffer.bytes = 131072\n",
      "\tsocket.connection.setup.timeout.max.ms = 30000\n",
      "\tsocket.connection.setup.timeout.ms = 10000\n",
      "\tssl.cipher.suites = null\n",
      "\tssl.enabled.protocols = [TLSv1.2, TLSv1.3]\n",
      "\tssl.endpoint.identification.algorithm = https\n",
      "\tssl.engine.factory.class = null\n",
      "\tssl.key.password = null\n",
      "\tssl.keymanager.algorithm = SunX509\n",
      "\tssl.keystore.certificate.chain = null\n",
      "\tssl.keystore.key = null\n",
      "\tssl.keystore.location = null\n",
      "\tssl.keystore.password = null\n",
      "\tssl.keystore.type = JKS\n",
      "\tssl.protocol = TLSv1.3\n",
      "\tssl.provider = null\n",
      "\tssl.secure.random.implementation = null\n",
      "\tssl.trustmanager.algorithm = PKIX\n",
      "\tssl.truststore.certificates = null\n",
      "\tssl.truststore.location = null\n",
      "\tssl.truststore.password = null\n",
      "\tssl.truststore.type = JKS\n",
      "\n",
      "23/06/16 10:13:19 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.\n",
      "23/06/16 10:13:19 INFO AppInfoParser: Kafka version: 3.3.2\n",
      "23/06/16 10:13:19 INFO AppInfoParser: Kafka commitId: b66af662e61082cb\n",
      "23/06/16 10:13:19 INFO AppInfoParser: Kafka startTimeMs: 1686890599243\n",
      "23/06/16 10:13:19 INFO AppInfoParser: App info kafka.admin.client for adminclient-1 unregistered\n",
      "23/06/16 10:13:19 INFO Metrics: Metrics scheduler closed\n",
      "23/06/16 10:13:19 INFO Metrics: Closing reporter org.apache.kafka.common.metrics.JmxReporter\n",
      "23/06/16 10:13:19 INFO Metrics: Metrics reporters closed\n",
      "23/06/16 10:13:19 INFO KafkaRelation: GetBatch generating RDD of offset range: KafkaOffsetRange(json-0,-2,-1,None)\n",
      "23/06/16 10:13:19 INFO CodeGenerator: Code generated in 19.226973 ms\n",
      "23/06/16 10:13:19 INFO CodeGenerator: Code generated in 20.132229 ms\n",
      "23/06/16 10:13:20 INFO CodeGenerator: Code generated in 13.695928 ms\n",
      "23/06/16 10:13:20 INFO SparkContext: Starting job: toPandas at /home/aryakumar/Downloads/frompyspark.py:28\n",
      "23/06/16 10:13:20 INFO DAGScheduler: Got job 0 (toPandas at /home/aryakumar/Downloads/frompyspark.py:28) with 1 output partitions\n",
      "23/06/16 10:13:20 INFO DAGScheduler: Final stage: ResultStage 0 (toPandas at /home/aryakumar/Downloads/frompyspark.py:28)\n",
      "23/06/16 10:13:20 INFO DAGScheduler: Parents of final stage: List()\n",
      "23/06/16 10:13:20 INFO DAGScheduler: Missing parents: List()\n",
      "23/06/16 10:13:20 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[10] at toPandas at /home/aryakumar/Downloads/frompyspark.py:28), which has no missing parents\n",
      "23/06/16 10:13:20 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 51.0 KiB, free 434.4 MiB)\n",
      "23/06/16 10:13:20 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 19.4 KiB, free 434.3 MiB)\n",
      "23/06/16 10:13:20 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on fedora:40889 (size: 19.4 KiB, free: 434.4 MiB)\n",
      "23/06/16 10:13:20 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1535\n",
      "23/06/16 10:13:20 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[10] at toPandas at /home/aryakumar/Downloads/frompyspark.py:28) (first 15 tasks are for partitions Vector(0))\n",
      "23/06/16 10:13:20 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0\n",
      "23/06/16 10:13:20 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (fedora, executor driver, partition 0, PROCESS_LOCAL, 7541 bytes) \n",
      "23/06/16 10:13:20 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
      "23/06/16 10:13:20 INFO ConsumerConfig: ConsumerConfig values: \n",
      "\tallow.auto.create.topics = true\n",
      "\tauto.commit.interval.ms = 5000\n",
      "\tauto.offset.reset = none\n",
      "\tbootstrap.servers = [localhost:9092]\n",
      "\tcheck.crcs = true\n",
      "\tclient.dns.lookup = use_all_dns_ips\n",
      "\tclient.id = consumer-spark-kafka-relation-99d6809d-bf53-4045-9778-9161852b1a0e-executor-1\n",
      "\tclient.rack = \n",
      "\tconnections.max.idle.ms = 540000\n",
      "\tdefault.api.timeout.ms = 60000\n",
      "\tenable.auto.commit = false\n",
      "\texclude.internal.topics = true\n",
      "\tfetch.max.bytes = 52428800\n",
      "\tfetch.max.wait.ms = 500\n",
      "\tfetch.min.bytes = 1\n",
      "\tgroup.id = spark-kafka-relation-99d6809d-bf53-4045-9778-9161852b1a0e-executor\n",
      "\tgroup.instance.id = null\n",
      "\theartbeat.interval.ms = 3000\n",
      "\tinterceptor.classes = []\n",
      "\tinternal.leave.group.on.close = true\n",
      "\tinternal.throw.on.fetch.stable.offset.unsupported = false\n",
      "\tisolation.level = read_uncommitted\n",
      "\tkey.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer\n",
      "\tmax.partition.fetch.bytes = 1048576\n",
      "\tmax.poll.interval.ms = 300000\n",
      "\tmax.poll.records = 500\n",
      "\tmetadata.max.age.ms = 300000\n",
      "\tmetric.reporters = []\n",
      "\tmetrics.num.samples = 2\n",
      "\tmetrics.recording.level = INFO\n",
      "\tmetrics.sample.window.ms = 30000\n",
      "\tpartition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]\n",
      "\treceive.buffer.bytes = 65536\n",
      "\treconnect.backoff.max.ms = 1000\n",
      "\treconnect.backoff.ms = 50\n",
      "\trequest.timeout.ms = 30000\n",
      "\tretry.backoff.ms = 100\n",
      "\tsasl.client.callback.handler.class = null\n",
      "\tsasl.jaas.config = null\n",
      "\tsasl.kerberos.kinit.cmd = /usr/bin/kinit\n",
      "\tsasl.kerberos.min.time.before.relogin = 60000\n",
      "\tsasl.kerberos.service.name = null\n",
      "\tsasl.kerberos.ticket.renew.jitter = 0.05\n",
      "\tsasl.kerberos.ticket.renew.window.factor = 0.8\n",
      "\tsasl.login.callback.handler.class = null\n",
      "\tsasl.login.class = null\n",
      "\tsasl.login.connect.timeout.ms = null\n",
      "\tsasl.login.read.timeout.ms = null\n",
      "\tsasl.login.refresh.buffer.seconds = 300\n",
      "\tsasl.login.refresh.min.period.seconds = 60\n",
      "\tsasl.login.refresh.window.factor = 0.8\n",
      "\tsasl.login.refresh.window.jitter = 0.05\n",
      "\tsasl.login.retry.backoff.max.ms = 10000\n",
      "\tsasl.login.retry.backoff.ms = 100\n",
      "\tsasl.mechanism = GSSAPI\n",
      "\tsasl.oauthbearer.clock.skew.seconds = 30\n",
      "\tsasl.oauthbearer.expected.audience = null\n",
      "\tsasl.oauthbearer.expected.issuer = null\n",
      "\tsasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000\n",
      "\tsasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000\n",
      "\tsasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100\n",
      "\tsasl.oauthbearer.jwks.endpoint.url = null\n",
      "\tsasl.oauthbearer.scope.claim.name = scope\n",
      "\tsasl.oauthbearer.sub.claim.name = sub\n",
      "\tsasl.oauthbearer.token.endpoint.url = null\n",
      "\tsecurity.protocol = PLAINTEXT\n",
      "\tsecurity.providers = null\n",
      "\tsend.buffer.bytes = 131072\n",
      "\tsession.timeout.ms = 45000\n",
      "\tsocket.connection.setup.timeout.max.ms = 30000\n",
      "\tsocket.connection.setup.timeout.ms = 10000\n",
      "\tssl.cipher.suites = null\n",
      "\tssl.enabled.protocols = [TLSv1.2, TLSv1.3]\n",
      "\tssl.endpoint.identification.algorithm = https\n",
      "\tssl.engine.factory.class = null\n",
      "\tssl.key.password = null\n",
      "\tssl.keymanager.algorithm = SunX509\n",
      "\tssl.keystore.certificate.chain = null\n",
      "\tssl.keystore.key = null\n",
      "\tssl.keystore.location = null\n",
      "\tssl.keystore.password = null\n",
      "\tssl.keystore.type = JKS\n",
      "\tssl.protocol = TLSv1.3\n",
      "\tssl.provider = null\n",
      "\tssl.secure.random.implementation = null\n",
      "\tssl.trustmanager.algorithm = PKIX\n",
      "\tssl.truststore.certificates = null\n",
      "\tssl.truststore.location = null\n",
      "\tssl.truststore.password = null\n",
      "\tssl.truststore.type = JKS\n",
      "\tvalue.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer\n",
      "\n",
      "23/06/16 10:13:20 INFO AppInfoParser: Kafka version: 3.3.2\n",
      "23/06/16 10:13:20 INFO AppInfoParser: Kafka commitId: b66af662e61082cb\n",
      "23/06/16 10:13:20 INFO AppInfoParser: Kafka startTimeMs: 1686890600541\n",
      "23/06/16 10:13:20 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-relation-99d6809d-bf53-4045-9778-9161852b1a0e-executor-1, groupId=spark-kafka-relation-99d6809d-bf53-4045-9778-9161852b1a0e-executor] Assigned to partition(s): json-0\n",
      "23/06/16 10:13:20 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-relation-99d6809d-bf53-4045-9778-9161852b1a0e-executor-1, groupId=spark-kafka-relation-99d6809d-bf53-4045-9778-9161852b1a0e-executor] Seeking to earliest offset of partition json-0\n",
      "23/06/16 10:13:20 INFO Metadata: [Consumer clientId=consumer-spark-kafka-relation-99d6809d-bf53-4045-9778-9161852b1a0e-executor-1, groupId=spark-kafka-relation-99d6809d-bf53-4045-9778-9161852b1a0e-executor] Resetting the last seen epoch of partition json-0 to 0 since the associated topicId changed from null to bmLLa_C1SGarmG4cxQK1zw\n",
      "23/06/16 10:13:20 INFO Metadata: [Consumer clientId=consumer-spark-kafka-relation-99d6809d-bf53-4045-9778-9161852b1a0e-executor-1, groupId=spark-kafka-relation-99d6809d-bf53-4045-9778-9161852b1a0e-executor] Cluster ID: AVPQrWCiQWCkO37yWphANg\n",
      "23/06/16 10:13:20 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-relation-99d6809d-bf53-4045-9778-9161852b1a0e-executor-1, groupId=spark-kafka-relation-99d6809d-bf53-4045-9778-9161852b1a0e-executor] Resetting offset for partition json-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.\n",
      "23/06/16 10:13:20 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-relation-99d6809d-bf53-4045-9778-9161852b1a0e-executor-1, groupId=spark-kafka-relation-99d6809d-bf53-4045-9778-9161852b1a0e-executor] Seeking to latest offset of partition json-0\n",
      "23/06/16 10:13:20 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-relation-99d6809d-bf53-4045-9778-9161852b1a0e-executor-1, groupId=spark-kafka-relation-99d6809d-bf53-4045-9778-9161852b1a0e-executor] Resetting offset for partition json-0 to position FetchPosition{offset=1000, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.\n",
      "23/06/16 10:13:20 INFO CodeGenerator: Code generated in 32.692283 ms\n",
      "23/06/16 10:13:20 INFO CodeGenerator: Code generated in 8.895059 ms\n",
      "23/06/16 10:13:20 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-relation-99d6809d-bf53-4045-9778-9161852b1a0e-executor-1, groupId=spark-kafka-relation-99d6809d-bf53-4045-9778-9161852b1a0e-executor] Seeking to offset 0 for partition json-0\n",
      "23/06/16 10:13:20 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-relation-99d6809d-bf53-4045-9778-9161852b1a0e-executor-1, groupId=spark-kafka-relation-99d6809d-bf53-4045-9778-9161852b1a0e-executor] Seeking to earliest offset of partition json-0\n",
      "23/06/16 10:13:20 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-relation-99d6809d-bf53-4045-9778-9161852b1a0e-executor-1, groupId=spark-kafka-relation-99d6809d-bf53-4045-9778-9161852b1a0e-executor] Resetting offset for partition json-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.\n",
      "23/06/16 10:13:20 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-relation-99d6809d-bf53-4045-9778-9161852b1a0e-executor-1, groupId=spark-kafka-relation-99d6809d-bf53-4045-9778-9161852b1a0e-executor] Seeking to latest offset of partition json-0\n",
      "23/06/16 10:13:20 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-relation-99d6809d-bf53-4045-9778-9161852b1a0e-executor-1, groupId=spark-kafka-relation-99d6809d-bf53-4045-9778-9161852b1a0e-executor] Resetting offset for partition json-0 to position FetchPosition{offset=1000, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.\n",
      "23/06/16 10:13:20 INFO CodeGenerator: Code generated in 53.582427 ms\n",
      "23/06/16 10:13:20 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-relation-99d6809d-bf53-4045-9778-9161852b1a0e-executor-1, groupId=spark-kafka-relation-99d6809d-bf53-4045-9778-9161852b1a0e-executor] Seeking to offset 500 for partition json-0\n",
      "23/06/16 10:13:21 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-relation-99d6809d-bf53-4045-9778-9161852b1a0e-executor-1, groupId=spark-kafka-relation-99d6809d-bf53-4045-9778-9161852b1a0e-executor] Seeking to earliest offset of partition json-0\n",
      "23/06/16 10:13:21 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-relation-99d6809d-bf53-4045-9778-9161852b1a0e-executor-1, groupId=spark-kafka-relation-99d6809d-bf53-4045-9778-9161852b1a0e-executor] Resetting offset for partition json-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.\n",
      "23/06/16 10:13:21 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-relation-99d6809d-bf53-4045-9778-9161852b1a0e-executor-1, groupId=spark-kafka-relation-99d6809d-bf53-4045-9778-9161852b1a0e-executor] Seeking to latest offset of partition json-0\n",
      "23/06/16 10:13:21 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-relation-99d6809d-bf53-4045-9778-9161852b1a0e-executor-1, groupId=spark-kafka-relation-99d6809d-bf53-4045-9778-9161852b1a0e-executor] Resetting offset for partition json-0 to position FetchPosition{offset=1000, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=0}}.\n",
      "23/06/16 10:13:21 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 14710 bytes result sent to driver\n",
      "23/06/16 10:13:21 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 760 ms on fedora (executor driver) (1/1)\n",
      "23/06/16 10:13:21 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
      "23/06/16 10:13:21 INFO DAGScheduler: ResultStage 0 (toPandas at /home/aryakumar/Downloads/frompyspark.py:28) finished in 0.920 s\n",
      "23/06/16 10:13:21 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "23/06/16 10:13:21 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished\n",
      "23/06/16 10:13:21 INFO DAGScheduler: Job 0 finished: toPandas at /home/aryakumar/Downloads/frompyspark.py:28, took 0.965248 s\n",
      "23/06/16 10:13:21 INFO CodeGenerator: Code generated in 6.764955 ms\n",
      "23/06/16 10:13:21 INFO SparkContext: Starting job: save at NativeMethodAccessorImpl.java:0\n",
      "23/06/16 10:13:21 INFO DAGScheduler: Got job 1 (save at NativeMethodAccessorImpl.java:0) with 8 output partitions\n",
      "23/06/16 10:13:21 INFO DAGScheduler: Final stage: ResultStage 1 (save at NativeMethodAccessorImpl.java:0)\n",
      "23/06/16 10:13:21 INFO DAGScheduler: Parents of final stage: List()\n",
      "23/06/16 10:13:21 INFO DAGScheduler: Missing parents: List()\n",
      "23/06/16 10:13:21 INFO DAGScheduler: Submitting ResultStage 1 (SQLExecutionRDD[18] at save at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "23/06/16 10:13:21 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 30.4 KiB, free 434.3 MiB)\n",
      "23/06/16 10:13:21 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 13.0 KiB, free 434.3 MiB)\n",
      "23/06/16 10:13:21 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on fedora:40889 (size: 13.0 KiB, free: 434.4 MiB)\n",
      "23/06/16 10:13:21 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1535\n",
      "23/06/16 10:13:21 INFO DAGScheduler: Submitting 8 missing tasks from ResultStage 1 (SQLExecutionRDD[18] at save at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7))\n",
      "23/06/16 10:13:21 INFO TaskSchedulerImpl: Adding task set 1.0 with 8 tasks resource profile 0\n",
      "23/06/16 10:13:21 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (fedora, executor driver, partition 0, PROCESS_LOCAL, 7869 bytes) \n",
      "23/06/16 10:13:21 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 2) (fedora, executor driver, partition 1, PROCESS_LOCAL, 7869 bytes) \n",
      "23/06/16 10:13:21 INFO TaskSetManager: Starting task 2.0 in stage 1.0 (TID 3) (fedora, executor driver, partition 2, PROCESS_LOCAL, 7869 bytes) \n",
      "23/06/16 10:13:21 INFO TaskSetManager: Starting task 3.0 in stage 1.0 (TID 4) (fedora, executor driver, partition 3, PROCESS_LOCAL, 7869 bytes) \n",
      "23/06/16 10:13:21 INFO TaskSetManager: Starting task 4.0 in stage 1.0 (TID 5) (fedora, executor driver, partition 4, PROCESS_LOCAL, 7869 bytes) \n",
      "23/06/16 10:13:21 INFO TaskSetManager: Starting task 5.0 in stage 1.0 (TID 6) (fedora, executor driver, partition 5, PROCESS_LOCAL, 7869 bytes) \n",
      "23/06/16 10:13:21 INFO TaskSetManager: Starting task 6.0 in stage 1.0 (TID 7) (fedora, executor driver, partition 6, PROCESS_LOCAL, 7869 bytes) \n",
      "23/06/16 10:13:21 INFO TaskSetManager: Starting task 7.0 in stage 1.0 (TID 8) (fedora, executor driver, partition 7, PROCESS_LOCAL, 7914 bytes) \n",
      "23/06/16 10:13:21 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)\n",
      "23/06/16 10:13:21 INFO Executor: Running task 1.0 in stage 1.0 (TID 2)\n",
      "23/06/16 10:13:21 INFO Executor: Running task 2.0 in stage 1.0 (TID 3)\n",
      "23/06/16 10:13:21 INFO Executor: Running task 3.0 in stage 1.0 (TID 4)\n",
      "23/06/16 10:13:21 INFO Executor: Running task 4.0 in stage 1.0 (TID 5)\n",
      "23/06/16 10:13:21 INFO Executor: Running task 5.0 in stage 1.0 (TID 6)\n",
      "23/06/16 10:13:21 INFO Executor: Running task 7.0 in stage 1.0 (TID 8)\n",
      "23/06/16 10:13:21 INFO Executor: Running task 6.0 in stage 1.0 (TID 7)\n",
      "23/06/16 10:13:22 INFO CodeGenerator: Code generated in 14.47255 ms\n",
      "23/06/16 10:13:22 INFO CodeGenerator: Code generated in 29.145396 ms\n",
      "23/06/16 10:13:22 INFO ProducerConfig: ProducerConfig values: \n",
      "\tacks = -1\n",
      "\tbatch.size = 16384\n",
      "\tbootstrap.servers = [localhost:9092]\n",
      "\tbuffer.memory = 33554432\n",
      "\tclient.dns.lookup = use_all_dns_ips\n",
      "\tclient.id = producer-1\n",
      "\tcompression.type = none\n",
      "\tconnections.max.idle.ms = 540000\n",
      "\tdelivery.timeout.ms = 120000\n",
      "\tenable.idempotence = true\n",
      "\tinterceptor.classes = []\n",
      "\tkey.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer\n",
      "\tlinger.ms = 0\n",
      "\tmax.block.ms = 60000\n",
      "\tmax.in.flight.requests.per.connection = 5\n",
      "\tmax.request.size = 1048576\n",
      "\tmetadata.max.age.ms = 300000\n",
      "\tmetadata.max.idle.ms = 300000\n",
      "\tmetric.reporters = []\n",
      "\tmetrics.num.samples = 2\n",
      "\tmetrics.recording.level = INFO\n",
      "\tmetrics.sample.window.ms = 30000\n",
      "\tpartitioner.adaptive.partitioning.enable = true\n",
      "\tpartitioner.availability.timeout.ms = 0\n",
      "\tpartitioner.class = null\n",
      "\tpartitioner.ignore.keys = false\n",
      "\treceive.buffer.bytes = 32768\n",
      "\treconnect.backoff.max.ms = 1000\n",
      "\treconnect.backoff.ms = 50\n",
      "\trequest.timeout.ms = 30000\n",
      "\tretries = 2147483647\n",
      "\tretry.backoff.ms = 100\n",
      "\tsasl.client.callback.handler.class = null\n",
      "\tsasl.jaas.config = null\n",
      "\tsasl.kerberos.kinit.cmd = /usr/bin/kinit\n",
      "\tsasl.kerberos.min.time.before.relogin = 60000\n",
      "\tsasl.kerberos.service.name = null\n",
      "\tsasl.kerberos.ticket.renew.jitter = 0.05\n",
      "\tsasl.kerberos.ticket.renew.window.factor = 0.8\n",
      "\tsasl.login.callback.handler.class = null\n",
      "\tsasl.login.class = null\n",
      "\tsasl.login.connect.timeout.ms = null\n",
      "\tsasl.login.read.timeout.ms = null\n",
      "\tsasl.login.refresh.buffer.seconds = 300\n",
      "\tsasl.login.refresh.min.period.seconds = 60\n",
      "\tsasl.login.refresh.window.factor = 0.8\n",
      "\tsasl.login.refresh.window.jitter = 0.05\n",
      "\tsasl.login.retry.backoff.max.ms = 10000\n",
      "\tsasl.login.retry.backoff.ms = 100\n",
      "\tsasl.mechanism = GSSAPI\n",
      "\tsasl.oauthbearer.clock.skew.seconds = 30\n",
      "\tsasl.oauthbearer.expected.audience = null\n",
      "\tsasl.oauthbearer.expected.issuer = null\n",
      "\tsasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000\n",
      "\tsasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000\n",
      "\tsasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100\n",
      "\tsasl.oauthbearer.jwks.endpoint.url = null\n",
      "\tsasl.oauthbearer.scope.claim.name = scope\n",
      "\tsasl.oauthbearer.sub.claim.name = sub\n",
      "\tsasl.oauthbearer.token.endpoint.url = null\n",
      "\tsecurity.protocol = PLAINTEXT\n",
      "\tsecurity.providers = null\n",
      "\tsend.buffer.bytes = 131072\n",
      "\tsocket.connection.setup.timeout.max.ms = 30000\n",
      "\tsocket.connection.setup.timeout.ms = 10000\n",
      "\tssl.cipher.suites = null\n",
      "\tssl.enabled.protocols = [TLSv1.2, TLSv1.3]\n",
      "\tssl.endpoint.identification.algorithm = https\n",
      "\tssl.engine.factory.class = null\n",
      "\tssl.key.password = null\n",
      "\tssl.keymanager.algorithm = SunX509\n",
      "\tssl.keystore.certificate.chain = null\n",
      "\tssl.keystore.key = null\n",
      "\tssl.keystore.location = null\n",
      "\tssl.keystore.password = null\n",
      "\tssl.keystore.type = JKS\n",
      "\tssl.protocol = TLSv1.3\n",
      "\tssl.provider = null\n",
      "\tssl.secure.random.implementation = null\n",
      "\tssl.trustmanager.algorithm = PKIX\n",
      "\tssl.truststore.certificates = null\n",
      "\tssl.truststore.location = null\n",
      "\tssl.truststore.password = null\n",
      "\tssl.truststore.type = JKS\n",
      "\ttransaction.timeout.ms = 60000\n",
      "\ttransactional.id = null\n",
      "\tvalue.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer\n",
      "\n",
      "23/06/16 10:13:22 INFO KafkaProducer: [Producer clientId=producer-1] Instantiated an idempotent producer.\n",
      "23/06/16 10:13:22 INFO AppInfoParser: Kafka version: 3.3.2\n",
      "23/06/16 10:13:22 INFO AppInfoParser: Kafka commitId: b66af662e61082cb\n",
      "23/06/16 10:13:22 INFO AppInfoParser: Kafka startTimeMs: 1686890602732\n",
      "23/06/16 10:13:22 INFO Metadata: [Producer clientId=producer-1] Cluster ID: AVPQrWCiQWCkO37yWphANg\n",
      "23/06/16 10:13:22 INFO TransactionManager: [Producer clientId=producer-1] ProducerId set to 1003 with epoch 0\n",
      "23/06/16 10:13:22 INFO Metadata: [Producer clientId=producer-1] Resetting the last seen epoch of partition spark-0 to 0 since the associated topicId changed from null to 2SF6FH2VQwGby3ftm886ow\n",
      "23/06/16 10:13:22 INFO PythonRunner: Times: total = 765, boot = 568, init = 197, finish = 0\n",
      "23/06/16 10:13:22 INFO PythonRunner: Times: total = 749, boot = 580, init = 169, finish = 0\n",
      "23/06/16 10:13:22 INFO PythonRunner: Times: total = 705, boot = 570, init = 135, finish = 0\n",
      "23/06/16 10:13:22 INFO PythonRunner: Times: total = 765, boot = 581, init = 184, finish = 0\n",
      "23/06/16 10:13:22 INFO PythonRunner: Times: total = 712, boot = 574, init = 138, finish = 0\n",
      "23/06/16 10:13:22 INFO PythonRunner: Times: total = 744, boot = 584, init = 160, finish = 0\n",
      "23/06/16 10:13:22 INFO PythonRunner: Times: total = 723, boot = 591, init = 132, finish = 0\n",
      "23/06/16 10:13:22 INFO PythonRunner: Times: total = 829, boot = 616, init = 212, finish = 1\n",
      "23/06/16 10:13:22 INFO Executor: Finished task 5.0 in stage 1.0 (TID 6). 1664 bytes result sent to driver\n",
      "23/06/16 10:13:22 INFO Executor: Finished task 7.0 in stage 1.0 (TID 8). 1664 bytes result sent to driver\n",
      "23/06/16 10:13:22 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1707 bytes result sent to driver\n",
      "23/06/16 10:13:22 INFO Executor: Finished task 2.0 in stage 1.0 (TID 3). 1664 bytes result sent to driver\n",
      "23/06/16 10:13:22 INFO Executor: Finished task 1.0 in stage 1.0 (TID 2). 1664 bytes result sent to driver\n",
      "23/06/16 10:13:22 INFO Executor: Finished task 3.0 in stage 1.0 (TID 4). 1707 bytes result sent to driver\n",
      "23/06/16 10:13:22 INFO Executor: Finished task 4.0 in stage 1.0 (TID 5). 1664 bytes result sent to driver\n",
      "23/06/16 10:13:22 INFO Executor: Finished task 6.0 in stage 1.0 (TID 7). 1664 bytes result sent to driver\n",
      "23/06/16 10:13:22 INFO TaskSetManager: Finished task 2.0 in stage 1.0 (TID 3) in 931 ms on fedora (executor driver) (1/8)\n",
      "23/06/16 10:13:22 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 2) in 932 ms on fedora (executor driver) (2/8)\n",
      "23/06/16 10:13:22 INFO TaskSetManager: Finished task 7.0 in stage 1.0 (TID 8) in 929 ms on fedora (executor driver) (3/8)\n",
      "23/06/16 10:13:22 INFO TaskSetManager: Finished task 3.0 in stage 1.0 (TID 4) in 932 ms on fedora (executor driver) (4/8)\n",
      "23/06/16 10:13:22 INFO TaskSetManager: Finished task 4.0 in stage 1.0 (TID 5) in 932 ms on fedora (executor driver) (5/8)\n",
      "23/06/16 10:13:22 INFO PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 58681\n",
      "23/06/16 10:13:22 INFO TaskSetManager: Finished task 6.0 in stage 1.0 (TID 7) in 931 ms on fedora (executor driver) (6/8)\n",
      "23/06/16 10:13:22 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 937 ms on fedora (executor driver) (7/8)\n",
      "23/06/16 10:13:22 INFO TaskSetManager: Finished task 5.0 in stage 1.0 (TID 6) in 935 ms on fedora (executor driver) (8/8)\n",
      "23/06/16 10:13:22 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
      "23/06/16 10:13:22 INFO DAGScheduler: ResultStage 1 (save at NativeMethodAccessorImpl.java:0) finished in 0.962 s\n",
      "23/06/16 10:13:22 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "23/06/16 10:13:22 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished\n",
      "23/06/16 10:13:22 INFO DAGScheduler: Job 1 finished: save at NativeMethodAccessorImpl.java:0, took 0.966438 s\n",
      "23/06/16 10:13:22 INFO BlockManagerInfo: Removed broadcast_0_piece0 on fedora:40889 in memory (size: 19.4 KiB, free: 434.4 MiB)\n",
      "23/06/16 10:13:23 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-relation-99d6809d-bf53-4045-9778-9161852b1a0e-executor-1, groupId=spark-kafka-relation-99d6809d-bf53-4045-9778-9161852b1a0e-executor] Resetting generation and member id due to: consumer pro-actively leaving the group\n",
      "23/06/16 10:13:23 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-relation-99d6809d-bf53-4045-9778-9161852b1a0e-executor-1, groupId=spark-kafka-relation-99d6809d-bf53-4045-9778-9161852b1a0e-executor] Request joining group due to: consumer pro-actively leaving the group\n",
      "23/06/16 10:13:23 INFO Metrics: Metrics scheduler closed\n",
      "23/06/16 10:13:23 INFO Metrics: Closing reporter org.apache.kafka.common.metrics.JmxReporter\n",
      "23/06/16 10:13:23 INFO Metrics: Metrics reporters closed\n",
      "23/06/16 10:13:23 INFO AppInfoParser: App info kafka.consumer for consumer-spark-kafka-relation-99d6809d-bf53-4045-9778-9161852b1a0e-executor-1 unregistered\n",
      "23/06/16 10:13:23 INFO SparkContext: Invoking stop() from shutdown hook\n",
      "23/06/16 10:13:23 INFO SparkContext: SparkContext is stopping with exitCode 0.\n",
      "23/06/16 10:13:23 INFO SparkUI: Stopped Spark web UI at http://fedora:4040\n",
      "23/06/16 10:13:23 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
      "23/06/16 10:13:23 INFO MemoryStore: MemoryStore cleared\n",
      "23/06/16 10:13:23 INFO BlockManager: BlockManager stopped\n",
      "23/06/16 10:13:23 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
      "23/06/16 10:13:23 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
      "23/06/16 10:13:23 INFO SparkContext: Successfully stopped SparkContext\n",
      "23/06/16 10:13:23 INFO ShutdownHookManager: Shutdown hook called\n",
      "23/06/16 10:13:23 INFO ShutdownHookManager: Deleting directory /tmp/spark-4f1bb807-fc28-4b1d-8a97-89822c121819\n",
      "23/06/16 10:13:23 INFO ShutdownHookManager: Deleting directory /tmp/spark-416fadf0-bf87-4a91-ba94-d124616a0ed7\n",
      "23/06/16 10:13:23 INFO ShutdownHookManager: Deleting directory /tmp/spark-416fadf0-bf87-4a91-ba94-d124616a0ed7/pyspark-24e216c5-590d-495b-87c1-6ebc204e331a\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "SPARK_COMMAND = \"/home/aryakumar/Downloads/spark-3.4.0-bin-hadoop3/bin/spark-submit \\\n",
    "    --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.4.0 \\\n",
    "                /home/aryakumar/Downloads/frompyspark.py\"\n",
    "\n",
    "os.system(SPARK_COMMAND)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tree = dfCoreSetTree(pandas_df, 200)\n",
    "Tree.fit()\n",
    "coreset = Tree.coreset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- x: double (nullable = true)\n",
      " |-- y: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.createDataFrame(coreset).printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "samle = df.sample(False,0.99, seed=0).orderBy(rand()).limit(1)\n",
    "samle = samle.withColumn('dummy', explode(array([lit(1)] * df.count()))).drop('dummy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+----------+\n",
      "|key|        Xr|        Yr|\n",
      "+---+----------+----------+\n",
      "|  0|0.08243975|-1.4575515|\n",
      "|  1|0.08243975|-1.4575515|\n",
      "|  2|0.08243975|-1.4575515|\n",
      "|  3|0.08243975|-1.4575515|\n",
      "|  4|0.08243975|-1.4575515|\n",
      "|  5|0.08243975|-1.4575515|\n",
      "|  6|0.08243975|-1.4575515|\n",
      "|  7|0.08243975|-1.4575515|\n",
      "|  8|0.08243975|-1.4575515|\n",
      "|  9|0.08243975|-1.4575515|\n",
      "| 10|0.08243975|-1.4575515|\n",
      "| 11|0.08243975|-1.4575515|\n",
      "| 12|0.08243975|-1.4575515|\n",
      "| 13|0.08243975|-1.4575515|\n",
      "| 14|0.08243975|-1.4575515|\n",
      "| 15|0.08243975|-1.4575515|\n",
      "| 16|0.08243975|-1.4575515|\n",
      "| 17|0.08243975|-1.4575515|\n",
      "| 18|0.08243975|-1.4575515|\n",
      "| 19|0.08243975|-1.4575515|\n",
      "+---+----------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "samle = samle.withColumn(\"key\", monotonically_increasing_id())\n",
    "samle = samle.selectExpr(\"CAST(key AS INT)\",\"CAST(x AS FLOAT)\", \"CAST(y AS FLOAT)\")\n",
    "samle = samle.withColumnRenamed('x', \"Xr\").withColumnRenamed('y', 'Yr')\n",
    "samle.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: integer (nullable = false)\n",
      " |-- Xr: float (nullable = true)\n",
      " |-- Yr: float (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "samle.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+-----------+---+-----------+----------+\n",
      "|key|          x|          y|key|         Xr|        Yr|\n",
      "+---+-----------+-----------+---+-----------+----------+\n",
      "|  0| 0.49671414| -0.1382643|  0|-0.46947438|0.54256004|\n",
      "|  1| 0.64768857|  1.5230298|  1|-0.46947438|0.54256004|\n",
      "|  2|-0.23415338|-0.23413695|  2|-0.46947438|0.54256004|\n",
      "|  3|  1.5792128|  0.7674347|  3|-0.46947438|0.54256004|\n",
      "|  4|-0.46947438| 0.54256004|  4|-0.46947438|0.54256004|\n",
      "|  5|-0.46341768|-0.46572974|  5|-0.46947438|0.54256004|\n",
      "|  6| 0.24196227| -1.9132802|  6|-0.46947438|0.54256004|\n",
      "|  7| -1.7249179| -0.5622875|  7|-0.46947438|0.54256004|\n",
      "|  8| -1.0128311| 0.31424734|  8|-0.46947438|0.54256004|\n",
      "|  9| -0.9080241| -1.4123037|  9|-0.46947438|0.54256004|\n",
      "+---+-----------+-----------+---+-----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mid = df.join(samle, df.key == samle.key, how='left'); mid.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: integer (nullable = true)\n",
      " |-- x: float (nullable = true)\n",
      " |-- y: float (nullable = true)\n",
      " |-- key: integer (nullable = true)\n",
      " |-- Xr: float (nullable = true)\n",
      " |-- Yr: float (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mid.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+-----------+---+-----------+----------+-----------+-----------+\n",
      "|key|          x|          y|key|         Xr|        Yr|          c|          d|\n",
      "+---+-----------+-----------+---+-----------+----------+-----------+-----------+\n",
      "|  0| 0.49671414| -0.1382643|  0|-0.46947438|0.54256004| 0.96618855|-0.68082434|\n",
      "|  1| 0.64768857|  1.5230298|  1|-0.46947438|0.54256004|   1.117163| 0.98046976|\n",
      "|  2|-0.23415338|-0.23413695|  2|-0.46947438|0.54256004|   0.235321|  -0.776697|\n",
      "|  3|  1.5792128|  0.7674347|  3|-0.46947438|0.54256004|  2.0486872| 0.22487468|\n",
      "|  4|-0.46947438| 0.54256004|  4|-0.46947438|0.54256004|        0.0|        0.0|\n",
      "|  5|-0.46341768|-0.46572974|  5|-0.46947438|0.54256004|0.006056696| -1.0082898|\n",
      "|  6| 0.24196227| -1.9132802|  6|-0.46947438|0.54256004|  0.7114366| -2.4558403|\n",
      "|  7| -1.7249179| -0.5622875|  7|-0.46947438|0.54256004| -1.2554436| -1.1048476|\n",
      "|  8| -1.0128311| 0.31424734|  8|-0.46947438|0.54256004| -0.5433567| -0.2283127|\n",
      "|  9| -0.9080241| -1.4123037|  9|-0.46947438|0.54256004| -0.4385497| -1.9548638|\n",
      "+---+-----------+-----------+---+-----------+----------+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = mid.withColumn('c', col(\"x\") - col(\"Xr\")).withColumn('d', col(\"y\") - col(\"Yr\"))\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.data.iris().to_csv('iris.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = pd.read_csv('file.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>key</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.496714</td>\n",
       "      <td>-0.138264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.647689</td>\n",
       "      <td>1.523030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>-0.234153</td>\n",
       "      <td>-0.234137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1.579213</td>\n",
       "      <td>0.767435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>-0.469474</td>\n",
       "      <td>0.542560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9994</th>\n",
       "      <td>9994</td>\n",
       "      <td>12.036582</td>\n",
       "      <td>-0.412744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>9995</td>\n",
       "      <td>11.232867</td>\n",
       "      <td>-0.468661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>9996</td>\n",
       "      <td>10.888839</td>\n",
       "      <td>0.902492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>9997</td>\n",
       "      <td>10.952591</td>\n",
       "      <td>0.387827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>9998</td>\n",
       "      <td>11.377367</td>\n",
       "      <td>0.378354</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9999 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       key          x         y\n",
       "0        0   0.496714 -0.138264\n",
       "1        1   0.647689  1.523030\n",
       "2        2  -0.234153 -0.234137\n",
       "3        3   1.579213  0.767435\n",
       "4        4  -0.469474  0.542560\n",
       "...    ...        ...       ...\n",
       "9994  9994  12.036582 -0.412744\n",
       "9995  9995  11.232867 -0.468661\n",
       "9996  9996  10.888839  0.902492\n",
       "9997  9997  10.952591  0.387827\n",
       "9998  9998  11.377367  0.378354\n",
       "\n",
       "[9999 rows x 3 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdcdp import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "npp = np.array(iris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "npp = npp[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9999, 2)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "npp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdd = PDCDP(7, 4)\n",
    "pdd.fit(npp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdd = pdd.labels[:, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris['labels'] = pdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.drop('key', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.to_csv('final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.496714</td>\n",
       "      <td>-0.138264</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.647689</td>\n",
       "      <td>1.523030</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.234153</td>\n",
       "      <td>-0.234137</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.579213</td>\n",
       "      <td>0.767435</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.469474</td>\n",
       "      <td>0.542560</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9994</th>\n",
       "      <td>12.036582</td>\n",
       "      <td>-0.412744</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>11.232867</td>\n",
       "      <td>-0.468661</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>10.888839</td>\n",
       "      <td>0.902492</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>10.952591</td>\n",
       "      <td>0.387827</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>11.377367</td>\n",
       "      <td>0.378354</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9999 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              x         y  labels\n",
       "0      0.496714 -0.138264       2\n",
       "1      0.647689  1.523030       2\n",
       "2     -0.234153 -0.234137       2\n",
       "3      1.579213  0.767435       2\n",
       "4     -0.469474  0.542560       2\n",
       "...         ...       ...     ...\n",
       "9994  12.036582 -0.412744       1\n",
       "9995  11.232867 -0.468661       1\n",
       "9996  10.888839  0.902492       1\n",
       "9997  10.952591  0.387827       1\n",
       "9998  11.377367  0.378354       1\n",
       "\n",
       "[9999 rows x 3 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[WRITE_STREAM_NOT_ALLOWED] `writeStream` can be called only on streaming Dataset/DataFrame.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[54], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m df \u001b[39m=\u001b[39m spark\u001b[39m.\u001b[39mcreateDataFrame(data, [\u001b[39m\"\u001b[39m\u001b[39mname\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mage\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m      5\u001b[0m \u001b[39m# convert the static dataframe to a streaming dataframe\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m streaming_df \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39mwriteStream\u001b[39m.\u001b[39mformat(\u001b[39m\"\u001b[39m\u001b[39mmemory\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mqueryName(\u001b[39m\"\u001b[39m\u001b[39mmy_table\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mstart()\n\u001b[1;32m      8\u001b[0m \u001b[39m# display the streaming dataframe\u001b[39;00m\n\u001b[1;32m      9\u001b[0m display(streaming_df)\n",
      "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.11/site-packages/pyspark/sql/dataframe.py:541\u001b[0m, in \u001b[0;36mDataFrame.writeStream\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[39m@property\u001b[39m\n\u001b[1;32m    513\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwriteStream\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataStreamWriter:\n\u001b[1;32m    514\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    515\u001b[0m \u001b[39m    Interface for saving the content of the streaming :class:`DataFrame` out into external\u001b[39;00m\n\u001b[1;32m    516\u001b[0m \u001b[39m    storage.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    539\u001b[0m \u001b[39m    <pyspark.sql.streaming.query.StreamingQuery object at 0x...>\u001b[39;00m\n\u001b[1;32m    540\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 541\u001b[0m     \u001b[39mreturn\u001b[39;00m DataStreamWriter(\u001b[39mself\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.11/site-packages/pyspark/sql/streaming/readwriter.py:739\u001b[0m, in \u001b[0;36mDataStreamWriter.__init__\u001b[0;34m(self, df)\u001b[0m\n\u001b[1;32m    737\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_df \u001b[39m=\u001b[39m df\n\u001b[1;32m    738\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_spark \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39msparkSession\n\u001b[0;32m--> 739\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jwrite \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39m_jdf\u001b[39m.\u001b[39mwriteStream()\n",
      "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.11/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_id, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname)\n\u001b[1;32m   1325\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(temp_arg, \u001b[39m\"\u001b[39m\u001b[39m_detach\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py:175\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    171\u001b[0m converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n\u001b[1;32m    172\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    173\u001b[0m     \u001b[39m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    174\u001b[0m     \u001b[39m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 175\u001b[0m     \u001b[39mraise\u001b[39;00m converted \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    176\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    177\u001b[0m     \u001b[39mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [WRITE_STREAM_NOT_ALLOWED] `writeStream` can be called only on streaming Dataset/DataFrame."
     ]
    }
   ],
   "source": [
    "\n",
    "# create a static dataframe\n",
    "data = [(\"Alice\", 1), (\"Bob\", 2), (\"Charlie\", 3)]\n",
    "df = spark.createDataFrame(data, [\"name\", \"age\"])\n",
    "\n",
    "# convert the static dataframe to a streaming dataframe\n",
    "streaming_df = df.writeStream.format(\"memory\").queryName(\"my_table\").start()\n",
    "\n",
    "# display the streaming dataframe\n",
    "display(streaming_df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
